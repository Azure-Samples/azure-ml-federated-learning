# example yaml config

# using this to store references to Azure ML
aml:
  # subscription_id: "<SUBSCRIPTION_ID>"
  # resource_group_name: "<RESOURCE_GROUP>"
  # workspace_name: "<AML_WORKSPACE_NAME>"

# federated learning parameters
federated_learning:
  orchestrator:
    compute: "orchestrator-01"
    datastore: "datastore_orchestrator"

  silos:
    - name: silo0
      computes: 
      - silo0-01 # name of the compute for silo X
      datastore: datastore_silo0
      silo_data:
        type: uri_folder
        mode: 'download'
        path: azureml://datastores/datastore_silo0/paths/federated_learning/pneumonia

    - name: silo1
      computes: 
      - silo1-01 # we are repeating over the same config for silo 2
      datastore: datastore_silo1
      silo_data:
        type: uri_folder
        mode: 'download'
        path: azureml://datastores/datastore_silo1/paths/federated_learning/pneumonia

    - name: silo2
      computes: 
      - silo2-01 # we are repeating over the same config for silo 3
      datastore: datastore_silo2
      silo_data:
        type: uri_folder
        mode: 'download'
        path: azureml://datastores/datastore_silo2/paths/federated_learning/pneumonia

# training parameters
training_parameters:
  num_of_iterations: 2
  epochs: 5
  lr: 0.01
  batch_size: 32

  # Differential privacy
  dp: false # Flag to enable/disable differential privacy
  dp_target_epsilon: 50.0 # Smaller epsilon means more privacy, more noise (it depends on the size of the training dataset. For more info, please visit https://opacus.ai/docs/faq#what-does-epsilon11-really-mean-how-about-delta )
  dp_target_delta: 1e-5 # The target δ of the (ϵ,δ)-differential privacy guarantee. Generally, it should be set to be less than the inverse of the size of the training dataset. 
  dp_max_grad_norm: 1.0 # Clip per-sample gradients to this norm (DP)

  # if you want to use the privacy_engine.make_private method, please set the value of dp_noise_multiplier parameter
  # dp_noise_multiplier: 1.0 # Noise multiplier - to add noise to gradients (DP)