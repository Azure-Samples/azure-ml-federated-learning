{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning basic pipeline using the FL API from the Azure ML SDK\n",
    "\n",
    "This notebook:\n",
    "1. reads a config file in yaml specifying the number of silos and their parameters,\n",
    "2. reads the components from a given folder,\n",
    "3. uses the Scatter-Gather API to build a flexible pipeline depending on the config, and to configure each step to read/write from the right storage account."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General imports\n",
    "\n",
    "Here, we import all the packages we'll need, except for the Azure packages. Further details below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "import webbrowser\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# to handle yaml config easily\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activate Private Preview features\n",
    "\n",
    "__This needs to happen *before* importing the Azure ML SDK.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_ML_CLI_PRIVATE_FEATURES_ENABLED\"] = \"True\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Imports\n",
    "\n",
    "Now we can import the AzureML SDK, **after** the environment variable above has been set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic Azure ML sdk v2 imports\n",
    "import azure\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "# FL-specific Azure ML sdk v2 imports\n",
    "import azure.ai.ml.dsl._fl_scatter_gather_node as fl\n",
    "from azure.ai.ml.entities._assets.federated_learning_silo import FederatedLearningSilo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the notebook\n",
    "\n",
    "This part is for reading the parameters from a config file, and for stating where we will look for the components. This is fairly standard in our FL examples and has nothing to do with the Scatter-Gather API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the example to run (only 'MNIST' is supported in this sample)\n",
    "example = \"MNIST\"\n",
    "\n",
    "# load the config from a local yaml file\n",
    "YAML_CONFIG = OmegaConf.load(\"./config.yaml\")\n",
    "\n",
    "# path to the components\n",
    "COMPONENTS_FOLDER = os.path.join(\n",
    "    \".\", \"..\", \"..\", \"components\", example\n",
    ")\n",
    "\n",
    "# path to the shared components\n",
    "SHARED_COMPONENTS_FOLDER = os.path.join(\n",
    "    \".\", \"..\", \"..\", \"components\", \"utils\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Azure ML\n",
    "\n",
    "This part is for connecting to the AzureML workspace. This is fairly standard in our FL examples and has nothing to do with the Scatter-Gather API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_aml():\n",
    "    try:\n",
    "        credential = DefaultAzureCredential()\n",
    "        # Check if given credential can get token successfully.\n",
    "        credential.get_token(\"https://management.azure.com/.default\")\n",
    "    except Exception as ex:\n",
    "        # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "        credential = InteractiveBrowserCredential()\n",
    "\n",
    "    # Get a handle to workspace\n",
    "    try:\n",
    "        # tries to connect using cli args if provided else using config.yaml\n",
    "        ML_CLIENT = MLClient(\n",
    "            subscription_id=YAML_CONFIG.aml.subscription_id,\n",
    "            resource_group_name=YAML_CONFIG.aml.resource_group_name,\n",
    "            workspace_name=YAML_CONFIG.aml.workspace_name,\n",
    "            credential=credential,\n",
    "        )\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Could not find either cli args or config.yaml.\")\n",
    "        # tries to connect using local config.json\n",
    "        ML_CLIENT = MLClient.from_config(credential=credential)\n",
    "\n",
    "    return ML_CLIENT\n",
    "\n",
    "print(\"Connecting to the Azure ML workspace...\")\n",
    "ML_CLIENT = connect_to_aml()\n",
    "print(\"Connected!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pipeline components\n",
    "\n",
    "This part is for loading the components we will be using in the pipeline. This is fairly standard in our FL examples and has nothing to do with the Scatter-Gather API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the component from their yaml specifications\n",
    "preprocessing_component = load_component(\n",
    "    source=os.path.join(COMPONENTS_FOLDER, \"preprocessing\", \"spec.yaml\")\n",
    ")\n",
    "\n",
    "training_component = load_component(\n",
    "    source=os.path.join(COMPONENTS_FOLDER, \"traininsilo\", \"spec.yaml\")\n",
    ")\n",
    "\n",
    "aggregate_component = load_component(\n",
    "    source=os.path.join(SHARED_COMPONENTS_FOLDER, \"aggregatemodelweights_mltable\", \"spec.yaml\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the silos\n",
    "\n",
    "Here we create a list containing the information for all silos, which will later on be passed to the Scatter-Gather API. We read the values from a config file.\n",
    "\n",
    "Note that using a config file is NOT mandatory. We usually find it more convenient to put all parameters in one file, but if you prefer you can also just create the list of silos directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silo_list = [\n",
    "    FederatedLearningSilo(\n",
    "        compute=silo_config[\"computes\"][0],\n",
    "        datastore=silo_config[\"datastore\"],\n",
    "        inputs= {\n",
    "            \"silo_name\": silo_config[\"name\"],\n",
    "            \"raw_train_data\": Input(**dict(silo_config[\"inputs\"])[\"training_data\"]),\n",
    "            \"raw_test_data\": Input(**dict(silo_config[\"inputs\"])[\"testing_data\"]),\n",
    "        },\n",
    "    )\n",
    "    for silo_config in YAML_CONFIG.strategy.horizontal\n",
    "]\n",
    "print(\"Silo list created\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create arguments mappings\n",
    "\n",
    "Create mappings for arguments - you should not have to modify anything in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silo_to_aggregation_argument_map = {\"model\" : \"from_silo_input\"}\n",
    "aggregation_to_silo_argument_map = {\"aggregated_output\" : \"checkpoint\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create kwargs inputs mappings\n",
    "\n",
    "Create mappings for inputs - you should not have to modify anything in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silo_kwargs = dict(YAML_CONFIG.silo_training_parameters)\n",
    "agg_kwargs = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create silo subgraph\n",
    "\n",
    "Here we build the subgraph defining what job(s) will happen in each silo. This will be passed to the Scatter-Gather API later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name=\"Silo Federated Learning Subgraph\",\n",
    "    description=\"It includes all steps that needs to be executing in silo\",\n",
    ")\n",
    "def silo_scatter_subgraph(\n",
    "    # user defined inputs\n",
    "    raw_train_data: Input,\n",
    "    raw_test_data: Input,\n",
    "    checkpoint: Input(optional=True),\n",
    "    silo_name: str,\n",
    "    # user defined training arguments\n",
    "    lr: float = 0.01,\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 64,\n",
    "    dp: bool = False,\n",
    "    dp_target_epsilon: float = 50.0,\n",
    "    dp_target_delta: float = 1e-5,\n",
    "    dp_max_grad_norm: float = 1.0,\n",
    ") -> dict:\n",
    "    \"\"\"Create scatter/silo subgraph.\n",
    "\n",
    "    Args:\n",
    "        raw_train_data (Input): raw train data\n",
    "        raw_test_data (Input): raw test data\n",
    "        checkpoint (Input): if not None, the checkpoint obtained from previous iteration\n",
    "        silo_name (str): name of the silo\n",
    "        lr (float, optional): Learning rate. Defaults to 0.01.\n",
    "        epochs (int, optional): Number of epochs. Defaults to 3.\n",
    "        batch_size (int, optional): Batch size. Defaults to 64.\n",
    "        dp (bool, optional): Differential Privacy\n",
    "        dp_target_epsilon (float, optional): DP target epsilon\n",
    "        dp_target_delta (float, optional): DP target delta\n",
    "        dp_max_grad_norm (float, optional): DP max gradient norm\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Outputs]: a map of the outputs\n",
    "    \"\"\"\n",
    "    # we're using our own preprocessing component\n",
    "    silo_pre_processing_step = preprocessing_component(\n",
    "        # this consumes whatever user defined inputs\n",
    "        raw_training_data=raw_train_data,\n",
    "        raw_testing_data=raw_test_data,\n",
    "        # here we're using the name of the silo compute as a metrics prefix\n",
    "        metrics_prefix=silo_name,\n",
    "    )\n",
    "\n",
    "    # we're using our own training component\n",
    "    silo_training_step = training_component(\n",
    "        # with the train_data from the pre_processing step\n",
    "        train_data=silo_pre_processing_step.outputs.processed_train_data,\n",
    "        # with the test_data from the pre_processing step\n",
    "        test_data=silo_pre_processing_step.outputs.processed_test_data,\n",
    "        # and the checkpoint from previous iteration (or None if iteration == 1)\n",
    "        checkpoint=checkpoint,\n",
    "        # Learning rate for local training\n",
    "        lr=lr,\n",
    "        # Number of epochs\n",
    "        epochs=epochs,\n",
    "        # Dataloader batch size\n",
    "        batch_size=batch_size,\n",
    "        # Differential Privacy\n",
    "        dp=dp,\n",
    "        # DP target epsilon\n",
    "        dp_target_epsilon=dp_target_epsilon,\n",
    "        # DP target delta\n",
    "        dp_target_delta=dp_target_delta,\n",
    "        # DP max gradient norm\n",
    "        dp_max_grad_norm=dp_max_grad_norm,\n",
    "        # Silo name/identifier\n",
    "        metrics_prefix=silo_name,\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: we will assume that any output provided here can be exfiltrated into the orchestrator/gather\n",
    "    return {\n",
    "        # NOTE: The key you use is custom. A mapping function needs to be provided to map the name here to the expected input from gather.\n",
    "        # This was already done, just above this function definition.\n",
    "\n",
    "        \"model\": silo_training_step.outputs.model\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the FL pipeline\n",
    "\n",
    "This is where the magic happens. We just use the `fl_scatter_gather` API, which will build the whole FL graph for us, and anchor the silos' components appropriately. See how we use the list of silos and subgraph we created earlier, along with the mappings, and some additional arguments from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_node = fl.fl_scatter_gather(\n",
    "    silo_configs=silo_list,\n",
    "    silo_component=silo_scatter_subgraph,\n",
    "    aggregation_component=aggregate_component,\n",
    "    aggregation_compute=YAML_CONFIG.orchestrator.compute,\n",
    "    aggregation_datastore=YAML_CONFIG.orchestrator.datastore,\n",
    "    shared_silo_kwargs=silo_kwargs,\n",
    "    aggregation_kwargs=agg_kwargs,\n",
    "    silo_to_aggregation_argument_map=silo_to_aggregation_argument_map,\n",
    "    aggregation_to_silo_argument_map=aggregation_to_silo_argument_map,\n",
    "    max_iterations=YAML_CONFIG.general_training_parameters.num_of_iterations,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the job\n",
    "\n",
    "This part is for submitting the job to AzureML. This is fairly standard in our FL examples and has nothing to do with the Scatter-Gather API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting the pipeline job to your AzureML workspace...\")\n",
    "pipeline_job = ML_CLIENT.jobs.create_or_update(\n",
    "    fl_node.scatter_gather_graph, experiment_name=\"example_fl_pipeline_with_sdk_accelerator\"\n",
    ")\n",
    "\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(pipeline_job.services[\"Studio\"].endpoint)\n",
    "\n",
    "webbrowser.open(pipeline_job.services[\"Studio\"].endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_sdk_preview_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00cd4189bac1922fab36716d9ab1cf01346cc830f046646060a75bdb11760bb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
